<p align="center">
  <h1 align="center">ğŸ”¤ Telugu Glyph Generation using VAEs</h1>
  <p align="center">
    <strong>Variational Autoencoders for Printed Telugu Character Synthesis</strong>
  </p>
  <p align="center">
    <a href="#-quick-start">Quick Start</a> â€¢
    <a href="#-model-architectures">Models</a> â€¢
    <a href="#-results">Results</a> â€¢
    <a href="#-documentation">Docs</a>
  </p>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-Academic-green.svg" alt="License">
  <img src="https://img.shields.io/badge/Status-Research-orange.svg" alt="Status">
</p>

---

## ğŸ“‹ Abstract

This research project implements and compares multiple **Variational Autoencoder (VAE)** architectures for generating high-quality printed **Telugu script** characters. Telugu, spoken by over 80 million people, has a complex Unicode structure with 72+ characters including vowels, consonants, and diacritical marks.

We evaluate four VAE variants with six different loss function configurations and demonstrate that **SSIM + Cyclical KL Annealing** achieves the best reconstruction quality with **0.9917 cosine similarity** on the test set.

**Key Contributions:**
- Systematic comparison of VAE architectures for Telugu character generation
- Novel loss function combination (SSIM + Cyclical KL Annealing)
- Open-source dataset and reproducible training pipeline

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Data Layer"
        A[Telugu Fonts<br/>Pothana2000.ttf] --> B[Glyph Renderer<br/>render_glyphs.py]
        B --> C[Augmentation Pipeline<br/>rotation, blur, noise]
        C --> D[(Dataset<br/>10,801 samples)]
    end
    
    subgraph "Model Layer"
        D --> E[DataLoader<br/>batch=16]
        E --> F{VAE Model}
        F --> G[Encoder<br/>Conv Layers]
        G --> H[Latent Space<br/>Î¼, ÏƒÂ²]
        H --> I[Reparameterize<br/>z = Î¼ + ÏƒÂ·Îµ]
        I --> J[Decoder<br/>ConvTranspose]
        J --> K[Reconstructed<br/>Image]
    end
    
    subgraph "Training Layer"
        K --> L[Loss Function<br/>BCE + KL + SSIM]
        L --> M[Optimizer<br/>Adam lr=0.0001]
        M --> N[Checkpoints<br/>best.pth]
    end
    
    subgraph "Evaluation Layer"
        N --> O[Sample Generation]
        N --> P[Latent Visualization<br/>t-SNE, PCA]
        N --> Q[Metrics<br/>Cosine Sim, FID]
    end
    
    style F fill:#e1f5fe
    style H fill:#fff3e0
    style L fill:#fce4ec
```

---

## ğŸ§  Model Architectures

### Architecture Comparison

| Model | Encoder | Latent Dim | Key Feature | Parameters |
|-------|---------|------------|-------------|------------|
| **VanillaVAE** | 3 Conv | 16 | Standard VAE | ~200K |
| **Î²-VAE** | 3 Conv | 16 | Weighted KL (Î²=4) | ~200K |
| **Conditional VAE** | 3 Conv + Embed | 16 | Class conditioning | ~250K |
| **Improved Î²-VAE** | 4 Residual | 16 | Skip connections + Attention | ~500K |

### Encoder-Decoder Architecture

```mermaid
graph LR
    subgraph "ENCODER"
        A[Input<br/>32Ã—32Ã—1] --> B[Conv 1â†’32<br/>stride=2]
        B --> C[Conv 32â†’64<br/>stride=2]
        C --> D[Conv 64â†’128<br/>stride=2]
        D --> E[Flatten<br/>2048]
        E --> F[FC â†’ Î¼<br/>16-dim]
        E --> G[FC â†’ logÏƒÂ²<br/>16-dim]
    end
    
    subgraph "LATENT"
        F --> H[Reparameterize]
        G --> H
        H --> I[z<br/>16-dim]
    end
    
    subgraph "DECODER"
        I --> J[FC<br/>2048]
        J --> K[Reshape<br/>4Ã—4Ã—128]
        K --> L[ConvT 128â†’64<br/>stride=2]
        L --> M[ConvT 64â†’32<br/>stride=2]
        M --> N[ConvT 32â†’1<br/>stride=2]
        N --> O[Output<br/>32Ã—32Ã—1]
    end
    
    style H fill:#fff3e0
    style I fill:#fff3e0
```

### Improved Î²-VAE with Residual Blocks

```mermaid
graph TB
    subgraph "Residual Block"
        X[Input x] --> C1[Conv 3Ã—3]
        C1 --> BN1[BatchNorm]
        BN1 --> R1[ReLU]
        R1 --> C2[Conv 3Ã—3]
        C2 --> BN2[BatchNorm]
        X --> SK[Skip<br/>1Ã—1 Conv if needed]
        BN2 --> ADD((+))
        SK --> ADD
        ADD --> R2[ReLU]
        R2 --> OUT[Output]
    end
```

---

## ğŸ“Š Training Pipeline

```mermaid
sequenceDiagram
    participant D as Dataset
    participant L as DataLoader
    participant M as VAE Model
    participant O as Optimizer
    participant C as Checkpoint
    
    Note over D,C: Training Loop (200 epochs)
    
    loop Each Epoch
        D->>L: Load batch (16 samples)
        L->>M: Forward pass
        M->>M: Encode â†’ z ~ q(z|x)
        M->>M: Decode â†’ x' = p(x|z)
        M->>O: Compute Loss<br/>L = BCE + Î²Â·KL + Î»Â·SSIM
        O->>M: Backward + Update weights
    end
    
    M->>C: Save best checkpoint
    
    Note over D,C: Evaluation Phase
    
    C->>M: Load best model
    M->>M: Generate samples
    M->>M: Compute test metrics
```

---

## ğŸ“ˆ Results

### Performance Summary

| Approach | Loss Function | Test Cosine Similarity | Rank |
|----------|---------------|------------------------|------|
| Baseline Î²-VAE | BCE + 4Â·KL | 0.9889 | 2nd |
| **SSIM + KL Annealing** | BCE + 0.3Â·SSIM + cyclical_KL | **0.9917** | ğŸ¥‡ **1st** |
| Combined Loss | L1 + SSIM + Focal + Cosine + KL | 0.9823 | 3rd |

### Why SSIM + KL Annealing Wins

```mermaid
pie title Performance Factors
    "SSIM Structural Loss" : 35
    "Cyclical KL Annealing" : 30
    "Balanced Weights" : 20
    "BCE Base Loss" : 15
```

**Key Insights:**
- âœ… **SSIM** preserves Telugu character stroke structure
- âœ… **Cyclical annealing** prevents posterior collapse
- âœ… **Moderate Î²** allows good reconstruction without over-regularization
- âŒ **Too many losses** (Approach 3) dilutes gradients

---

## ğŸ“ Project Structure

```
vae_project/
â”œâ”€â”€ ğŸ“‚ models/                      # VAE implementations
â”‚   â”œâ”€â”€ vae.py                      # VanillaVAE, BetaVAE, cVAE
â”‚   â”œâ”€â”€ improved_vae.py             # Residual blocks + Attention
â”‚   â”œâ”€â”€ networks.py                 # Encoder/Decoder networks
â”‚   â””â”€â”€ losses.py                   # Loss functions
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                     # Training & evaluation
â”‚   â”œâ”€â”€ train.py                    # Main training script
â”‚   â”œâ”€â”€ generate_samples.py         # Sample generation
â”‚   â”œâ”€â”€ latent_visualizer.py        # t-SNE, UMAP, traversals
â”‚   â””â”€â”€ evaluate.py                 # Evaluation metrics
â”‚
â”œâ”€â”€ ğŸ“‚ configs/                     # YAML configurations
â”‚   â””â”€â”€ beta_vae_baseline.yaml
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # Datasets
â”‚   â”œâ”€â”€ Pothana2000.ttf             # Telugu font
â”‚   â”œâ”€â”€ Vowel_Dataset/              # 6 vowel classes
â”‚   â””â”€â”€ metadata.csv                # Dataset metadata
â”‚
â”œâ”€â”€ ğŸ“‚ experiments/                 # Experiment outputs
â”œâ”€â”€ ğŸ“‚ results/                     # Metrics & reports
â”œâ”€â”€ ğŸ“‚ checkpoints/                 # Model weights
â”‚
â”œâ”€â”€ ğŸ“„ DOCUMENTATION.md             # Comprehensive docs
â”œâ”€â”€ ğŸ“„ train_vowel_experiments.py   # Main experiment script
â””â”€â”€ ğŸ“„ README.md                    # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8+
pip install torch torchvision matplotlib pandas scikit-learn \
    opencv-python pillow tqdm seaborn
```

### Train a Model

```bash
# Approach 1: Baseline Î²-VAE
python train_vowel_experiments.py --approach 1 --epochs 200

# Approach 2: SSIM + KL Annealing (BEST)
python train_vowel_experiments.py --approach 2 --epochs 200

# Approach 3: Combined Loss
python train_vowel_experiments.py --approach 3 --epochs 200

# All approaches
python train_vowel_experiments.py --approach 0
```

### Generate Samples

```bash
python scripts/generate_samples.py \
    --model_path checkpoints/vowel_approach_2/best.pth \
    --model_type beta_vae \
    --latent_dim 16 \
    --num_samples 100 \
    --output_dir results/generated_samples
```

### Visualize Latent Space

```bash
python scripts/latent_visualizer.py \
    --model_path checkpoints/vowel_approach_2/best.pth \
    --data_path data/Vowel_Dataset \
    --latent_dim 16
```

---

## ğŸ“Š Class Diagram

```mermaid
classDiagram
    class VanillaVAE {
        +latent_dim: int
        +encoder: ConvEncoder
        +decoder: ConvDecoder
        +encode(x) tuple
        +decode(z) Tensor
        +reparameterize(mu, logvar) Tensor
        +forward(x) tuple
        +loss(recon_x, x, mu, logvar)
        +sample(num_samples)
    }
    
    class BetaVAE {
        +beta: float
        +set_beta(beta)
    }
    
    class ConditionalVAE {
        +num_classes: int
        +encode(x, c)
        +decode(z, c)
        +forward(x, c)
        +sample(num_samples, class_id)
    }
    
    class ImprovedBetaVAE {
        +use_attention: bool
        -hidden_dims: list
        +count_parameters()
    }
    
    VanillaVAE <|-- BetaVAE : inherits
    VanillaVAE <|-- ConditionalVAE : extends
    VanillaVAE <|-- ImprovedBetaVAE : enhanced
    
    class ConvEncoder {
        +in_channels: int
        +latent_dim: int
        +hidden_dims: list
        +forward(x) tuple
    }
    
    class ConvDecoder {
        +latent_dim: int
        +out_channels: int
        +forward(z) Tensor
    }
    
    VanillaVAE --> ConvEncoder : has
    VanillaVAE --> ConvDecoder : has
```

---

## ğŸ“š Loss Functions

| Loss | Formula | Use Case |
|------|---------|----------|
| **BCE** | `-âˆ‘[xÂ·log(x') + (1-x)Â·log(1-x')]` | Pixel reconstruction |
| **KL Divergence** | `-0.5Â·âˆ‘(1 + logÏƒÂ² - Î¼Â² - ÏƒÂ²)` | Latent regularization |
| **SSIM** | `1 - SSIM(x, x')` | Structural similarity |
| **Focal BCE** | `Î±(1-p)^Î³ Â· BCE` | Hard sample mining |
| **L1/MAE** | `\|x - x'\|` | Edge preservation |
| **Cosine** | `1 - cos(x, x')` | Feature alignment |

---

## ğŸ“– Documentation

For comprehensive technical documentation, see:

ğŸ“„ **[DOCUMENTATION.md](./DOCUMENTATION.md)** - Full research documentation including:
- Problem statement & scope
- Dataset description & statistics
- Model architecture details
- Loss function analysis
- Experimental methodology
- Detailed results analysis
- Conclusions & future work

---

## ğŸ¯ Future Work

- [ ] Implement VQ-VAE for discrete latents
- [ ] Add more Telugu fonts (Vemana, Gautami)
- [ ] Compute FID scores
- [ ] Train OCR classifier for evaluation
- [ ] Extend to handwritten characters
- [ ] GPU training optimization

---

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{pamidi2025teluguvae,
  title={Telugu Glyph Generation using Variational Autoencoders},
  author={Pamidi, Rohit},
  year={2025},
  institution={Indian Institute of Technology},
  note={BTP Research Project}
}
```

---

## ğŸ‘¥ Contributors

- **Rohit Pamidi** - Primary Developer & Researcher
- **Faculty Advisor** - Project Guidance

---

## ğŸ“„ License

This project is for academic research purposes. Please cite if using this code or methodology.

---

<p align="center">
  <strong>ğŸ™ Thank you for exploring Telugu VAE Generation!</strong>
</p>
